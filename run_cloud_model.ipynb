{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_cloud_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkforward/I_know_the_cloud/blob/master/run_cloud_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-p1qwRY34Gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e95e0f8a-8298-4d5e-cdbf-bf261babdeff"
      },
      "source": [
        "!pip install catalyst\n",
        "!pip install pretrainedmodels\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install pytorch_toolbelt\n",
        "!pip install torchvision==0.4"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catalyst in /usr/local/lib/python3.6/dist-packages (19.11)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.17.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.2.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.9)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.37.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst) (5.5.0)\n",
            "Requirement already satisfied: GitPython>=2.1.11 in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.0.4)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.1.1.26)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.15.0)\n",
            "Requirement already satisfied: safitty>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.3)\n",
            "Requirement already satisfied: crc32c>=1.7 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.1.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.13)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.15.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.9.0)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.21.3)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.25.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->catalyst) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->catalyst) (1.12.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.4.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (41.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.7.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.3.3)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.1.11->catalyst) (2.0.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->catalyst) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst) (1.3.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst) (1.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->catalyst) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->catalyst) (0.33.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->catalyst) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->catalyst) (3.1.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->catalyst) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst) (0.2.0)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=2.1.11->catalyst) (2.0.5)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.2.1->catalyst) (0.46)\n",
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (2.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.37.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.17.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->pretrainedmodels) (0.46)\n",
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-74iy7fhz\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-74iy7fhz\n",
            "Requirement already satisfied (use --upgrade to upgrade): segmentation-models-pytorch==0.0.3 from git+https://github.com/qubvel/segmentation_models.pytorch in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: torchvision<=0.4.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.0.3) (0.4.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.0.3) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.4.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.0.3) (0.4.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (from torchvision<=0.4.0,>=0.2.2->segmentation-models-pytorch==0.0.3) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision<=0.4.0,>=0.2.2->segmentation-models-pytorch==0.0.3) (1.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision<=0.4.0,>=0.2.2->segmentation-models-pytorch==0.0.3) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision<=0.4.0,>=0.2.2->segmentation-models-pytorch==0.0.3) (4.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.0.3) (4.37.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.0.3) (2.5.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision<=0.4.0,>=0.2.2->segmentation-models-pytorch==0.0.3) (0.46)\n",
            "Building wheels for collected packages: segmentation-models-pytorch\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.0.3-cp36-none-any.whl size=29990 sha256=ac34a87bbfa620f67e8764bd935c5995dddc87c5f9145198178f1e2e1fc341d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2kjm_qi/wheels/79/3f/09/1587a252e0314d26ad242d6d2e165622ab95c95e5cfe4b942c\n",
            "Successfully built segmentation-models-pytorch\n",
            "Requirement already satisfied: pytorch_toolbelt in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_toolbelt) (4.1.1.26)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_toolbelt) (1.2.0)\n",
            "Requirement already satisfied: torchvision>=0.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_toolbelt) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.0->pytorch_toolbelt) (1.17.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3->pytorch_toolbelt) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3->pytorch_toolbelt) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3->pytorch_toolbelt) (0.46)\n",
            "Requirement already satisfied: torchvision==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (1.12.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (1.17.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.4) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDtD6jPT3aCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import collections\n",
        "import time \n",
        "import tqdm\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "train_on_gpu = True\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "import albumentations as albu\n",
        "from albumentations import torch as AT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Yw7T7N4Dql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catalyst.data import Augmentor\n",
        "from catalyst.dl import utils\n",
        "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from catalyst.contrib.models.segmentation import Unet\n",
        "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
        "\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdgtkS7H7J1G",
        "colab_type": "text"
      },
      "source": [
        "# Mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECF3WDad4p8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b15abf36-7e88-4bf1-94f7-6d8fa2c0a9e5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKv4AVM_7Zuj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa6d5924-683e-463a-9cbe-3b3f28bb7df6"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlTlA8EZI4Ds",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEQmbzQZ78Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_img(x, folder: str='train_images'):\n",
        "    \"\"\"\n",
        "    Return image based on image name and folder.\n",
        "    \"\"\"\n",
        "    data_folder = f\"{path}/{folder}\"\n",
        "    image_path = os.path.join(data_folder, x)\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "def rle_decode(mask_rle: str = '', shape: tuple = (1400, 2100)):\n",
        "    '''\n",
        "    Decode rle encoded mask.\n",
        "    \n",
        "    :param mask_rle: run-length as string formatted (start length)\n",
        "    :param shape: (height, width) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape, order='F')\n",
        "\n",
        "\n",
        "def make_mask(df: pd.DataFrame, image_name: str='img.jpg', shape: tuple = (1400, 2100)):\n",
        "    \"\"\"\n",
        "    Create mask based on df, image name and shape.\n",
        "    \"\"\"\n",
        "    encoded_masks = df.loc[df['im_id'] == image_name, 'EncodedPixels']\n",
        "    masks = np.zeros((shape[0], shape[1], 4), dtype=np.float32)\n",
        "\n",
        "    for idx, label in enumerate(encoded_masks.values):\n",
        "        if label is not np.nan:\n",
        "            mask = rle_decode(label)\n",
        "            masks[:, :, idx] = mask\n",
        "            \n",
        "    return masks\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    \"\"\"\n",
        "    Convert image or mask.\n",
        "    \"\"\"\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def mask2rle(img):\n",
        "    '''\n",
        "    Convert mask to rle.\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels= img.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "\n",
        "def visualize(image, mask, original_image=None, original_mask=None):\n",
        "    \"\"\"\n",
        "    Plot image and masks.\n",
        "    If two pairs of images and masks are passes, show both.\n",
        "    \"\"\"\n",
        "    fontsize = 14\n",
        "    class_dict = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\n",
        "    \n",
        "    if original_image is None and original_mask is None:\n",
        "        f, ax = plt.subplots(1, 5, figsize=(24, 24))\n",
        "\n",
        "        ax[0].imshow(image)\n",
        "        for i in range(4):\n",
        "            ax[i + 1].imshow(mask[:, :, i])\n",
        "            ax[i + 1].set_title(f'Mask {class_dict[i]}', fontsize=fontsize)\n",
        "    else:\n",
        "        f, ax = plt.subplots(2, 5, figsize=(24, 12))\n",
        "\n",
        "        ax[0, 0].imshow(original_image)\n",
        "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "                \n",
        "        for i in range(4):\n",
        "            ax[0, i + 1].imshow(original_mask[:, :, i])\n",
        "            ax[0, i + 1].set_title(f'Original mask {class_dict[i]}', fontsize=fontsize)\n",
        "        \n",
        "        ax[1, 0].imshow(image)\n",
        "        ax[1, 0].set_title('Transformed image', fontsize=fontsize)\n",
        "        \n",
        "        \n",
        "        for i in range(4):\n",
        "            ax[1, i + 1].imshow(mask[:, :, i])\n",
        "            ax[1, i + 1].set_title(f'Transformed mask {class_dict[i]}', fontsize=fontsize)\n",
        "            \n",
        "            \n",
        "def visualize_with_raw(image, mask, original_image=None, original_mask=None, raw_image=None, raw_mask=None):\n",
        "    \"\"\"\n",
        "    Plot image and masks.\n",
        "    If two pairs of images and masks are passes, show both.\n",
        "    \"\"\"\n",
        "    fontsize = 14\n",
        "    class_dict = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\n",
        "\n",
        "    f, ax = plt.subplots(3, 5, figsize=(24, 12))\n",
        "\n",
        "    ax[0, 0].imshow(original_image)\n",
        "    ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[0, i + 1].imshow(original_mask[:, :, i])\n",
        "        ax[0, i + 1].set_title(f'Original mask {class_dict[i]}', fontsize=fontsize)\n",
        "\n",
        "\n",
        "    ax[1, 0].imshow(raw_image)\n",
        "    ax[1, 0].set_title('Original image', fontsize=fontsize)\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[1, i + 1].imshow(raw_mask[:, :, i])\n",
        "        ax[1, i + 1].set_title(f'Raw predicted mask {class_dict[i]}', fontsize=fontsize)\n",
        "        \n",
        "    ax[2, 0].imshow(image)\n",
        "    ax[2, 0].set_title('Transformed image', fontsize=fontsize)\n",
        "\n",
        "\n",
        "    for i in range(4):\n",
        "        ax[2, i + 1].imshow(mask[:, :, i])\n",
        "        ax[2, i + 1].set_title(f'Predicted mask with processing {class_dict[i]}', fontsize=fontsize)\n",
        "            \n",
        "            \n",
        "def plot_with_augmentation(image, mask, augment):\n",
        "    \"\"\"\n",
        "    Wrapper for `visualize` function.\n",
        "    \"\"\"\n",
        "    augmented = augment(image=image, mask=mask)\n",
        "    image_flipped = augmented['image']\n",
        "    mask_flipped = augmented['mask']\n",
        "    visualize(image_flipped, mask_flipped, original_image=image, original_mask=mask)\n",
        "    \n",
        "    \n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def post_process(probability, threshold, min_size):\n",
        "    \"\"\"\n",
        "    Post processing of each predicted mask, components with lesser number of pixels\n",
        "    than `min_size` are ignored\n",
        "    \"\"\"\n",
        "    # don't remember where I saw it\n",
        "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
        "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
        "    predictions = np.zeros((350, 525), np.float32)\n",
        "    num = 0\n",
        "    for c in range(1, num_component):\n",
        "        p = (component == c)\n",
        "        if p.sum() > min_size:\n",
        "            predictions[p] = 1\n",
        "            num += 1\n",
        "    return predictions, num\n",
        "\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=0),\n",
        "        albu.GridDistortion(p=0.5),\n",
        "        albu.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5),\n",
        "        albu.Resize(320, 640)\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.Resize(320, 640)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n",
        "\n",
        "\n",
        "def dice(img1, img2):\n",
        "    img1 = np.asarray(img1).astype(np.bool)\n",
        "    img2 = np.asarray(img2).astype(np.bool)\n",
        "\n",
        "    intersection = np.logical_and(img1, img2)\n",
        "\n",
        "    return 2. * intersection.sum() / (img1.sum() + img2.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Ya9bUAKjun",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr4oyR37Ke2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/gdrive/My Drive/kaggle_cloud/data'\n",
        "\n",
        "\n",
        "def get_data(path):\n",
        "  train = pd.read_csv(f'{path}/train.csv')\n",
        "  sub = pd.read_csv(f'{path}/sample_submission.csv')\n",
        "  print(\"Reading the training csv...\")\n",
        "  print(train.columns)\n",
        "  print(train.shape)\n",
        "\n",
        "  n_train = len(os.listdir(f'{path}/train_images'))\n",
        "  n_test = len(os.listdir(f'{path}/test_images'))\n",
        "  print(\"Reading the training images...\")\n",
        "  print(f'There are {n_train} images in train dataset')\n",
        "  print(f'There are {n_test} images in test dataset')\n",
        "\n",
        "  n_samples = 400\n",
        "  train = train.iloc[:n_samples, :]\n",
        "\n",
        "\n",
        "  train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n",
        "  train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "  sub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\n",
        "  sub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "  return train, sub\n",
        "\n",
        "\n",
        "def display_images_with_masks(path, train):\n",
        "\tfig = plt.figure(figsize=(25, 16))\n",
        "\tfor j, im_id in enumerate(np.random.choice(train['im_id'].unique(), 4)):\n",
        "\t    for i, (idx, row) in enumerate(train.loc[train['im_id'] == im_id].iterrows()):\n",
        "\t        ax = fig.add_subplot(5, 4, j * 4 + i + 1, xticks=[], yticks=[])\n",
        "\t        im = Image.open(f\"{path}/train_images/{row['Image_Label'].split('_')[0]}\")\n",
        "\t        plt.imshow(im)\n",
        "\t        mask_rle = row['EncodedPixels']\n",
        "\t        try: # label might not be there!\n",
        "\t            mask = rle_decode(mask_rle)\n",
        "\t        except:\n",
        "\t            mask = np.zeros((1400, 2100))\n",
        "\t        plt.imshow(mask, alpha=0.5, cmap='gray')\n",
        "\t        ax.set_title(f\"Image: {row['Image_Label'].split('_')[0]}. Label: {row['label']}\")\n",
        "\n",
        "\n",
        "def split_data(train, sub):\n",
        "\tid_mask_count = train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().\\\n",
        "\t\t\t\treset_index().rename(columns={'index': 'img_id', 'Image_Label': 'count'})\n",
        "\ttrain_ids, valid_ids = train_test_split(id_mask_count['img_id'].values, random_state=42, stratify=id_mask_count['count'], test_size=0.1)\n",
        "\ttest_ids = sub['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values\n",
        "\n",
        "\treturn train_ids, valid_ids, test_ids\n",
        "\n",
        "\n",
        "class CloudDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame = None, datatype: str = 'train', img_ids: np.array = None,\n",
        "                 transforms = albu.Compose([albu.HorizontalFlip(),AT.ToTensor()]),\n",
        "                 preprocessing=None):\n",
        "        self.df = df\n",
        "        if datatype != 'test':\n",
        "            self.data_folder = f\"{path}/train_images\"\n",
        "        else:\n",
        "            self.data_folder = f\"{path}/test_images\"\n",
        "        self.img_ids = img_ids\n",
        "        self.transforms = transforms\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.img_ids[idx]\n",
        "        mask = make_mask(self.df, image_name)\n",
        "        image_path = os.path.join(self.data_folder, image_name)\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        augmented = self.transforms(image=img, mask=mask)\n",
        "        img = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        if self.preprocessing:\n",
        "            preprocessed = self.preprocessing(image=img, mask=mask)\n",
        "            img = preprocessed['image']\n",
        "            mask = preprocessed['mask']\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_uzRUA6K1Tk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "76582607-b63f-4530-88ba-83606f0c5427"
      },
      "source": [
        "train = pd.read_csv('{}/train.csv'.format(path), engine='python')\n",
        "sub = pd.read_csv('{}/sample_submission.csv'.format(path), engine='python')\n",
        "print(train.shape)\n",
        "print(sub.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22184, 2)\n",
            "(14792, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8MGjXL8Lhhs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "719f8df0-cdd5-4ea8-902a-12386e5600e2"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_Label</th>\n",
              "      <th>EncodedPixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0011165.jpg_Fish</td>\n",
              "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0011165.jpg_Flower</td>\n",
              "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0011165.jpg_Gravel</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0011165.jpg_Sugar</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>002be4f.jpg_Fish</td>\n",
              "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Image_Label                                      EncodedPixels\n",
              "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
              "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
              "2  0011165.jpg_Gravel                                                NaN\n",
              "3   0011165.jpg_Sugar                                                NaN\n",
              "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j2btHiSPSfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}