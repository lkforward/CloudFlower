{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkforward/flower/blob/master/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsTeKghC1OuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "a6819dff-2f42-4f6c-e6a2-fb04d9f6a2b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m_DT0iY2W58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "dbac438c-60b1-47a1-f358-54e2286d493b"
      },
      "source": [
        "!git clone https://github.com/vlievin/Unet.git /content/gdrive/My\\ Drive/kaggle_cloud/code_myunet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/kaggle_cloud/code_myunet'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYMo3pFQ2hDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a2f36f0-b9e6-4037-a70b-da4f4da8b79e"
      },
      "source": [
        "!ln -sfn /content/gdrive/My\\ Drive/kaggle_cloud/code_myunet code\n",
        "!ls code"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LICENSE  README.md  unet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnfOMnr62003",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat code/unet.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBhm5q254e8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class gated_resnet(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Residual Block\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters, kernel_size, padding, nonlinearity=nn.ReLU, dropout=0.2, dilation=1,batchNormObject=nn.BatchNorm2d):\n",
        "        super(gated_resnet, self).__init__()\n",
        "        self.gated = True\n",
        "        num_hidden_filters =2 * num_filters if gated else num_filters\n",
        "        self.conv_input = nn.Conv2d(num_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.nonlinearity = nonlinearity()\n",
        "        self.batch_norm1 = batchNormObject(num_hidden_filters)\n",
        "        self.conv_out = nn.Conv2d(num_hidden_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.batch_norm2 = batchNormObject(num_filters)\n",
        "\n",
        "    def forward(self, og_x):\n",
        "        x = self.conv_input(og_x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv_out(x)\n",
        "        if self.gated:\n",
        "            a, b = torch.chunk(x, 2, dim=1)\n",
        "            c3 = a * F.sigmoid(b)\n",
        "        else:\n",
        "            c3 = x\n",
        "        out = og_x + c3\n",
        "        out = self.batch_norm2(out)\n",
        "        return out\n",
        "    \n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters, kernel_size, padding, nonlinearity=nn.ReLU, dropout=0.2, dilation=1,batchNormObject=nn.BatchNorm2d):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        num_hidden_filters = num_filters\n",
        "        self.conv1 = nn.Conv2d(num_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.nonlinearity = nonlinearity(inplace=False)\n",
        "        self.batch_norm1 = batchNormObject(num_hidden_filters)\n",
        "        self.conv2 = nn.Conv2d(num_hidden_filters, num_hidden_filters, kernel_size=kernel_size,stride=1,padding=padding,dilation=dilation )\n",
        "        self.batch_norm2 = batchNormObject(num_filters)\n",
        "\n",
        "    def forward(self, og_x):\n",
        "        x = og_x\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv1(og_x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.nonlinearity(x)\n",
        "        x = self.conv2(x)\n",
        "        out = og_x + x\n",
        "        out = self.batch_norm2(out)\n",
        "        out = self.nonlinearity(out)\n",
        "        return out\n",
        "    \n",
        "class ConvolutionalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Encoder providing skip connections\n",
        "    \"\"\"\n",
        "    def __init__(self,n_features_input,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=0,dropout_max=0.2, blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        \"\"\"\n",
        "        n_features_input (int): number of intput features\n",
        "        num_hidden_features (list(int)): number of features for each stage\n",
        "        kernel_size (int): convolution kernel size\n",
        "        padding (int): convolution padding\n",
        "        n_resblocks (int): number of residual blocks at each stage\n",
        "        dropout (float): dropout probability\n",
        "        blockObject (nn.Module): Residual block to use. Default is ResidualBlock\n",
        "        batchNormObject (nn.Module): normalization layer. Default is nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(ConvolutionalEncoder,self).__init__()\n",
        "        self.n_features_input = n_features_input\n",
        "        self.num_hidden_features = num_hidden_features\n",
        "        self.stages = nn.ModuleList()\n",
        "        dropout = iter([(1-t)*dropout_min + t*dropout_max   for t in np.linspace(0,1,(len(num_hidden_features)))])\n",
        "        dropout = iter(dropout)\n",
        "        # input convolution block\n",
        "        block = [nn.Conv2d(n_features_input, num_hidden_features[0], kernel_size=kernel_size,stride=1, padding=padding)]\n",
        "        for _ in range(n_resblocks):\n",
        "            p = next(iter(dropout))\n",
        "            block += [blockObject(num_hidden_features[0], kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "        self.stages.append(nn.Sequential(*block))\n",
        "        # layers\n",
        "        for features_in,features_out in [num_hidden_features[i:i+2] for i in range(0,len(num_hidden_features), 1)][:-1]:\n",
        "            # downsampling\n",
        "            block = [nn.MaxPool2d(2),nn.Conv2d(features_in, features_out, kernel_size=1,padding=0 ),batchNormObject(features_out),nn.ReLU()]\n",
        "            #block = [nn.Conv2d(features_in, features_out, kernel_size=kernel_size,stride=2,padding=padding ),nn.BatchNorm2d(features_out),nn.ReLU()]\n",
        "            # residual blocks\n",
        "            p = next(iter(dropout))\n",
        "            for _ in range(n_resblocks):\n",
        "                block += [blockObject(features_out, kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "            self.stages.append(nn.Sequential(*block)) \n",
        "            \n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            skips.append(x)\n",
        "        return x,skips\n",
        "    def getInputShape(self):\n",
        "        return (-1,self.n_features_input,-1,-1)\n",
        "    def getOutputShape(self):\n",
        "        return (-1,self.num_hidden_features[-1], -1,-1)\n",
        "    \n",
        "            \n",
        "class ConvolutionalDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Decoder taking skip connections\n",
        "    \"\"\"\n",
        "    def __init__(self,n_features_output,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=0,dropout_max=0.2,blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        \"\"\"\n",
        "        n_features_output (int): number of output features\n",
        "        num_hidden_features (list(int)): number of features for each stage\n",
        "        kernel_size (int): convolution kernel size\n",
        "        padding (int): convolution padding\n",
        "        n_resblocks (int): number of residual blocks at each stage\n",
        "        dropout (float): dropout probability\n",
        "        blockObject (nn.Module): Residual block to use. Default is ResidualBlock\n",
        "        batchNormObject (nn.Module): normalization layer. Default is nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(ConvolutionalDecoder,self).__init__()\n",
        "        self.n_features_output = n_features_output\n",
        "        self.num_hidden_features = num_hidden_features\n",
        "        self.upConvolutions = nn.ModuleList()\n",
        "        self.skipMergers = nn.ModuleList()\n",
        "        self.residualBlocks = nn.ModuleList()\n",
        "        dropout = iter([(1-t)*dropout_min + t*dropout_max   for t in np.linspace(0,1,(len(num_hidden_features)))][::-1])\n",
        "        # input convolution block\n",
        "        # layers\n",
        "        for features_in,features_out in [num_hidden_features[i:i+2] for i in range(0,len(num_hidden_features), 1)][:-1]:\n",
        "            # downsampling\n",
        "            self.upConvolutions.append(nn.Sequential(nn.ConvTranspose2d(features_in, features_out, kernel_size=3, stride=2,padding=1,output_padding=1),batchNormObject(features_out),nn.ReLU()))\n",
        "            self.skipMergers.append(nn.Conv2d(2*features_out, features_out, kernel_size=kernel_size,stride=1, padding=padding))\n",
        "            # residual blocks\n",
        "            block = []\n",
        "            p = next(iter(dropout))\n",
        "            for _ in range(n_resblocks):\n",
        "                block += [blockObject(features_out, kernel_size, padding, dropout=p,batchNormObject=batchNormObject)]\n",
        "            self.residualBlocks.append(nn.Sequential(*block))   \n",
        "        # output convolution block\n",
        "        block = [nn.Conv2d(num_hidden_features[-1],n_features_output, kernel_size=kernel_size,stride=1, padding=padding)]\n",
        "        self.output_convolution = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self,x, skips):\n",
        "        for up,merge,conv,skip in zip(self.upConvolutions,self.skipMergers, self.residualBlocks,skips):\n",
        "            x = up(x)\n",
        "            cat = torch.cat([x,skip],1)\n",
        "            x = merge(cat)\n",
        "            x = conv(x)\n",
        "        return self.output_convolution(x)\n",
        "    def getInputShape(self):\n",
        "        return (-1,self.num_hidden_features[0],-1,-1)\n",
        "    def getOutputShape(self):\n",
        "        return (-1,self.n_features_output, -1,-1)\n",
        "    \n",
        "    \n",
        "class DilatedConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequential Dialted convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, n_convolutions, dropout):\n",
        "        super(DilatedConvolutions, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.non_linearity = nn.ReLU(inplace=True)\n",
        "        self.strides = [2**(k+1) for k in range(n_convolutions)]\n",
        "        convs = [nn.Conv2d(n_channels, n_channels, kernel_size=kernel_size,dilation=s, padding=s) for s in self.strides ]\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for c in convs:\n",
        "            self.convs.append(c)\n",
        "            self.bns.append(nn.BatchNorm2d(n_channels))\n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for (c,bn,s) in zip(self.convs,self.bns,self.strides):\n",
        "            x_in = x\n",
        "            x = c(x)\n",
        "            x = bn(x)\n",
        "            x = self.non_linearity(x)\n",
        "            x = self.dropout(x)\n",
        "            x = x_in + x\n",
        "            skips.append(x)\n",
        "        return x,skips\n",
        "    \n",
        "class DilatedConvolutions2(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequential Dialted convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, n_convolutions,dropout,kernel_size,blockObject=ResidualBlock,batchNormObject=nn.BatchNorm2d):\n",
        "        super(DilatedConvolutions2, self).__init__()\n",
        "        self.dilatations = [2**(k+1) for k in range(n_convolutions)]\n",
        "        self.blocks = nn.ModuleList([blockObject(n_channels, kernel_size, d, dropout=dropout, dilation=d,batchNormObject=batchNormObject) for d in self.dilatations ])\n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "            skips.append(x)\n",
        "        return x, skips\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net model with dynamic number of layers, Residual Blocks, Dilated Convolutions, Dropout and Group Normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_hidden_features,n_resblocks,num_dilated_convs, dropout_min=0, dropout_max=0, gated=False, padding=1, kernel_size=3,group_norm=32):\n",
        "        \"\"\"\n",
        "        initialize the model\n",
        "        Args:\n",
        "            in_channels (int): number of input channels (image=3)\n",
        "            out_channels (int): number of output channels (n_classes)\n",
        "            num_hidden_features (list(int)): number of hidden features for each layer (the number of layer is the lenght of this list)\n",
        "            n_resblocks (int): number of residual blocks at each layer \n",
        "            num_dilated_convs (int): number of dilated convolutions at the last layer\n",
        "            dropout (float): float in [0,1]: dropout probability\n",
        "            gated (bool): use gated Convolutions, default is False\n",
        "            padding (int): padding for the convolutions\n",
        "            kernel_size (int): kernel size for the convolutions\n",
        "            group_norm (bool): number of groups to use for Group Normalization, default is 32, if zero: use nn.BatchNorm2d\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        if group_norm > 0:\n",
        "            for h in num_hidden_features:\n",
        "                assert h%group_norm==0, \"Number of features at each layer must be divisible by 'group_norm'\"\n",
        "        blockObject = gated_resnet if gated else ResidualBlock\n",
        "        batchNormObject = lambda n_features : nn.GroupNorm(group_norm,n_features) if group_norm > 0 else nn.BatchNorm2d\n",
        "        self.encoder = ConvolutionalEncoder(in_channels,num_hidden_features,kernel_size,padding,n_resblocks,dropout_min=dropout_min,dropout_max=dropout_max,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "        if num_dilated_convs > 0:\n",
        "            #self.dilatedConvs = DilatedConvolutions2(num_hidden_features[-1], num_dilated_convs,dropout_max,kernel_size,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "            self.dilatedConvs = DilatedConvolutions(num_hidden_features[-1],num_dilated_convs,dropout_max) # <v11 uses dilatedConvs2\n",
        "        else:\n",
        "            self.dilatedConvs = None\n",
        "        self.decoder = ConvolutionalDecoder(out_channels,num_hidden_features[::-1],kernel_size,padding,n_resblocks,dropout_min=dropout_min,dropout_max=dropout_max,blockObject=blockObject,batchNormObject=batchNormObject)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x,skips = self.encoder(x)\n",
        "        if self.dilatedConvs is not None:\n",
        "            x,dilated_skips = self.dilatedConvs(x)\n",
        "            for d in dilated_skips:\n",
        "                x += d\n",
        "            x += skips[-1]\n",
        "        x = self.decoder(x,skips[:-1][::-1])\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxZ-bpNo4mLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}