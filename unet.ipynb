{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1lflLzVRFKBM",
        "WxosKwgsEyMv"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lkforward/flower/blob/master/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsTeKghC1OuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e26927b3-03d7-4af6-bc87-336bb23ecce9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "code  gdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lflLzVRFKBM",
        "colab_type": "text"
      },
      "source": [
        "# Load the U-Net Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m_DT0iY2W58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "69b211cc-66ae-46b7-f2b5-393e0a626f4c"
      },
      "source": [
        "# #Clone the code into google drive: \n",
        "\n",
        "# Codebase 1:\n",
        "# !git clone https://github.com/vlievin/Unet.git /content/gdrive/My\\ Drive/kaggle_cloud/code_myunet\n",
        "\n",
        "# Codebase 2:\n",
        "# !git clone https://github.com/lyakaap/Kaggle-Carvana-3rd-Place-Solution.git /content/gdrive/My\\ Drive/kaggle_cloud/code_3rdplace_unet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/kaggle_cloud/code_3rdplace_unet'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYMo3pFQ2hDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7accdf32-813a-4e51-d3ba-1c8842dc60c2"
      },
      "source": [
        "# !ln -sfn /content/gdrive/My\\ Drive/kaggle_cloud/code_myunet code\n",
        "# !ln -sfn /content/gdrive/My\\ Drive/kaggle_cloud/code_3rdplace_unet code\n",
        "!ls code"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "losses.py\t    model.py\t      network.png  train.py\n",
            "make_submission.py  model_pytorch.py  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnfOMnr62003",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #If you want to view the code from a pop-up window: \n",
        "# %pycat code/unet.py\n",
        "# %pycat code/model_pytorch.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV9N-MkH1rRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%writefile code/model_pytorch.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvActivation(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=1, dilation=1,\n",
        "                 activation=nn.ReLU(inplace=True)):\n",
        "        super(ConvActivation, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride, padding, dilation)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=1, dilation=1,\n",
        "                 activation=nn.ReLU(inplace=True)):\n",
        "        super(ConvBNActivation, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride, padding, dilation)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=1, dilation=1,\n",
        "                 batch_norm=False, activation=nn.ReLU(inplace=True)):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        conv = ConvBNActivation if batch_norm else ConvActivation\n",
        "        self.block = nn.Sequential(\n",
        "            conv(in_channels, out_channels, kernel_size,\n",
        "                 stride, padding, dilation, activation),\n",
        "            conv(out_channels, out_channels, kernel_size,\n",
        "                 stride, padding, dilation, activation)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpBlockWithSkip(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=1, dilation=1, up_mode='deconv',\n",
        "                 batch_norm=False, activation=nn.ReLU(inplace=True)):\n",
        "        assert up_mode in ('deconv', 'biupconv', 'nnupconv')\n",
        "        super(UpBlockWithSkip, self).__init__()\n",
        "\n",
        "        if up_mode == 'deconv':\n",
        "            self.up = nn.ConvTranspose2d(\n",
        "                in_channels, out_channels,\n",
        "                kernel_size=4, stride=2, padding=1)\n",
        "        elif up_mode == 'biupconv':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2,\n",
        "                            align_corners=False),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            )\n",
        "        elif up_mode == 'nnupconv':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='nearest', scale_factor=2,\n",
        "                            align_corners=False),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            )\n",
        "\n",
        "        self.conv_block = ConvBlock(\n",
        "            out_channels * 2, out_channels, kernel_size,\n",
        "            stride, padding, dilation, batch_norm, activation)\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        out = torch.cat([up, bridge], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DilatedUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, classes=1, depth=3,\n",
        "                 first_channels=44, padding=1,\n",
        "                 bottleneck_depth=6, bottleneck_type='cascade',\n",
        "                 batch_norm=False, up_mode='deconv',\n",
        "                 activation=nn.ReLU(inplace=True)):\n",
        "\n",
        "        assert bottleneck_type in ('cascade', 'parallel')\n",
        "        super(DilatedUNet, self).__init__()\n",
        "\n",
        "        self.depth = depth\n",
        "        self.bottleneck_type = bottleneck_type\n",
        "\n",
        "        conv = ConvBNActivation if batch_norm else ConvActivation\n",
        "\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                ConvBlock(prev_channels, first_channels * 2**i, 3,\n",
        "                          padding=padding, batch_norm=batch_norm,\n",
        "                          activation=activation))\n",
        "            prev_channels = first_channels * 2**i\n",
        "\n",
        "        self.bottleneck_path = nn.ModuleList()\n",
        "        for i in range(bottleneck_depth):\n",
        "            bneck_in = prev_channels if i == 0 else prev_channels * 2\n",
        "            self.bottleneck_path.append(\n",
        "                conv(bneck_in, prev_channels * 2, 3,\n",
        "                     dilation=2**i, padding=2**i, activation=activation))\n",
        "\n",
        "        prev_channels *= 2\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth)):\n",
        "            self.up_path.append(\n",
        "                UpBlockWithSkip(prev_channels, first_channels * 2**i, 3,\n",
        "                                up_mode=up_mode, padding=padding,\n",
        "                                batch_norm=batch_norm,\n",
        "                                activation=activation))\n",
        "            prev_channels = first_channels * 2**i\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bridges = []\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            bridges.append(x)\n",
        "            x = F.avg_pool2d(x, 2)\n",
        "\n",
        "        dilated_layers = []\n",
        "        for i, bneck in enumerate(self.bottleneck_path):\n",
        "            if self.bottleneck_type == 'cascade':\n",
        "                x = bneck(x)\n",
        "                dilated_layers.append(x.unsqueeze(-1))\n",
        "            elif self.bottleneck_type == 'parallel':\n",
        "                dilated_layers.append(bneck(x.unsqueeze(-1)))\n",
        "        x = torch.cat(dilated_layers, dim=-1)\n",
        "        x = torch.sum(x, dim=-1)\n",
        "\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, bridges[-i-1])\n",
        "\n",
        "        return self.last(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxZ-bpNo4mLM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55c6de78-f24f-4c94-8f83-01b4cb846568"
      },
      "source": [
        "# net = UNet(in_channels=3, out_channels=4, num_hidden_features=[64, 128, 256], \n",
        "#            num_dilated_convs=2, n_resblocks=2, \n",
        "#            dropout_min=0., dropout_max=0.2)\n",
        "\n",
        "net = DilatedUNet(in_channels=3, classes=4, depth=5)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DilatedUNet(\n",
            "  (down_path): ModuleList(\n",
            "    (0): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): ConvActivation(\n",
            "          (conv): Conv2d(3, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ConvActivation(\n",
            "          (conv): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): ConvActivation(\n",
            "          (conv): Conv2d(44, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ConvActivation(\n",
            "          (conv): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): ConvActivation(\n",
            "          (conv): Conv2d(88, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ConvActivation(\n",
            "          (conv): Conv2d(176, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): ConvActivation(\n",
            "          (conv): Conv2d(176, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ConvActivation(\n",
            "          (conv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): ConvActivation(\n",
            "          (conv): Conv2d(352, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ConvActivation(\n",
            "          (conv): Conv2d(704, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (activation): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bottleneck_path): ModuleList(\n",
            "    (0): ConvActivation(\n",
            "      (conv): Conv2d(704, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ConvActivation(\n",
            "      (conv): Conv2d(1408, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ConvActivation(\n",
            "      (conv): Conv2d(1408, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): ConvActivation(\n",
            "      (conv): Conv2d(1408, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): ConvActivation(\n",
            "      (conv): Conv2d(1408, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): ConvActivation(\n",
            "      (conv): Conv2d(1408, 1408, kernel_size=(3, 3), stride=(1, 1), padding=(32, 32), dilation=(32, 32))\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (up_path): ModuleList(\n",
            "    (0): UpBlockWithSkip(\n",
            "      (up): ConvTranspose2d(1408, 704, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (conv_block): ConvBlock(\n",
            "        (block): Sequential(\n",
            "          (0): ConvActivation(\n",
            "            (conv): Conv2d(1408, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ConvActivation(\n",
            "            (conv): Conv2d(704, 704, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): UpBlockWithSkip(\n",
            "      (up): ConvTranspose2d(704, 352, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (conv_block): ConvBlock(\n",
            "        (block): Sequential(\n",
            "          (0): ConvActivation(\n",
            "            (conv): Conv2d(704, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ConvActivation(\n",
            "            (conv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): UpBlockWithSkip(\n",
            "      (up): ConvTranspose2d(352, 176, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (conv_block): ConvBlock(\n",
            "        (block): Sequential(\n",
            "          (0): ConvActivation(\n",
            "            (conv): Conv2d(352, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ConvActivation(\n",
            "            (conv): Conv2d(176, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): UpBlockWithSkip(\n",
            "      (up): ConvTranspose2d(176, 88, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (conv_block): ConvBlock(\n",
            "        (block): Sequential(\n",
            "          (0): ConvActivation(\n",
            "            (conv): Conv2d(176, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ConvActivation(\n",
            "            (conv): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): UpBlockWithSkip(\n",
            "      (up): ConvTranspose2d(88, 44, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (conv_block): ConvBlock(\n",
            "        (block): Sequential(\n",
            "          (0): ConvActivation(\n",
            "            (conv): Conv2d(88, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ConvActivation(\n",
            "            (conv): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (last): Conv2d(44, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWbxhIkrmSl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf2fc0e1-aebe-45c7-b656-6238396ad7a0"
      },
      "source": [
        "input = torch.randn(16, 3, 320, 320)\n",
        "output = net(input)\n",
        "print(output)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.0226,  0.0278,  0.0194,  ...,  0.0214,  0.0171,  0.0257],\n",
            "          [ 0.0236,  0.0196,  0.0312,  ...,  0.0274,  0.0282,  0.0258],\n",
            "          [ 0.0210,  0.0256,  0.0272,  ...,  0.0295,  0.0239,  0.0250],\n",
            "          ...,\n",
            "          [ 0.0208,  0.0270,  0.0110,  ...,  0.0273,  0.0222,  0.0279],\n",
            "          [ 0.0267,  0.0278,  0.0314,  ...,  0.0371,  0.0219,  0.0300],\n",
            "          [ 0.0270,  0.0299,  0.0247,  ...,  0.0242,  0.0230,  0.0287]],\n",
            "\n",
            "         [[-0.1230, -0.1218, -0.1299,  ..., -0.1290, -0.1358, -0.1313],\n",
            "          [-0.1323, -0.1300, -0.1306,  ..., -0.1283, -0.1337, -0.1271],\n",
            "          [-0.1315, -0.1158, -0.1212,  ..., -0.1350, -0.1235, -0.1312],\n",
            "          ...,\n",
            "          [-0.1283, -0.1232, -0.1355,  ..., -0.1260, -0.1171, -0.1308],\n",
            "          [-0.1209, -0.1230, -0.1258,  ..., -0.1160, -0.1266, -0.1274],\n",
            "          [-0.1280, -0.1287, -0.1275,  ..., -0.1268, -0.1234, -0.1243]],\n",
            "\n",
            "         [[ 0.0389,  0.0436,  0.0301,  ...,  0.0441,  0.0437,  0.0436],\n",
            "          [ 0.0440,  0.0477,  0.0461,  ...,  0.0453,  0.0492,  0.0440],\n",
            "          [ 0.0409,  0.0506,  0.0440,  ...,  0.0450,  0.0439,  0.0446],\n",
            "          ...,\n",
            "          [ 0.0454,  0.0469,  0.0448,  ...,  0.0530,  0.0367,  0.0424],\n",
            "          [ 0.0479,  0.0508,  0.0515,  ...,  0.0417,  0.0426,  0.0532],\n",
            "          [ 0.0474,  0.0554,  0.0539,  ...,  0.0517,  0.0535,  0.0452]],\n",
            "\n",
            "         [[ 0.0300,  0.0281,  0.0314,  ...,  0.0301,  0.0348,  0.0335],\n",
            "          [ 0.0380,  0.0375,  0.0416,  ...,  0.0312,  0.0397,  0.0331],\n",
            "          [ 0.0390,  0.0410,  0.0320,  ...,  0.0460,  0.0385,  0.0300],\n",
            "          ...,\n",
            "          [ 0.0437,  0.0367,  0.0364,  ...,  0.0313,  0.0385,  0.0359],\n",
            "          [ 0.0472,  0.0341,  0.0309,  ...,  0.0376,  0.0313,  0.0352],\n",
            "          [ 0.0413,  0.0434,  0.0425,  ...,  0.0410,  0.0402,  0.0323]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0177,  0.0225,  0.0257,  ...,  0.0221,  0.0214,  0.0242],\n",
            "          [ 0.0289,  0.0263,  0.0212,  ...,  0.0173,  0.0204,  0.0198],\n",
            "          [ 0.0244,  0.0298,  0.0205,  ...,  0.0274,  0.0256,  0.0175],\n",
            "          ...,\n",
            "          [ 0.0327,  0.0209,  0.0274,  ...,  0.0162,  0.0180,  0.0316],\n",
            "          [ 0.0205,  0.0273,  0.0239,  ...,  0.0182,  0.0270,  0.0290],\n",
            "          [ 0.0302,  0.0340,  0.0227,  ...,  0.0222,  0.0223,  0.0284]],\n",
            "\n",
            "         [[-0.1281, -0.1281, -0.1245,  ..., -0.1269, -0.1347, -0.1288],\n",
            "          [-0.1232, -0.1252, -0.1364,  ..., -0.1227, -0.1247, -0.1379],\n",
            "          [-0.1215, -0.1243, -0.1293,  ..., -0.1218, -0.1158, -0.1324],\n",
            "          ...,\n",
            "          [-0.1219, -0.1221, -0.1094,  ..., -0.1255, -0.1221, -0.1276],\n",
            "          [-0.1301, -0.1304, -0.1201,  ..., -0.1293, -0.1330, -0.1234],\n",
            "          [-0.1266, -0.1210, -0.1296,  ..., -0.1311, -0.1275, -0.1239]],\n",
            "\n",
            "         [[ 0.0354,  0.0435,  0.0441,  ...,  0.0389,  0.0403,  0.0430],\n",
            "          [ 0.0514,  0.0455,  0.0482,  ...,  0.0371,  0.0412,  0.0485],\n",
            "          [ 0.0471,  0.0445,  0.0415,  ...,  0.0426,  0.0455,  0.0476],\n",
            "          ...,\n",
            "          [ 0.0500,  0.0541,  0.0539,  ...,  0.0410,  0.0501,  0.0415],\n",
            "          [ 0.0439,  0.0547,  0.0626,  ...,  0.0422,  0.0453,  0.0382],\n",
            "          [ 0.0512,  0.0553,  0.0520,  ...,  0.0553,  0.0549,  0.0445]],\n",
            "\n",
            "         [[ 0.0300,  0.0318,  0.0288,  ...,  0.0288,  0.0291,  0.0305],\n",
            "          [ 0.0366,  0.0394,  0.0341,  ...,  0.0302,  0.0315,  0.0276],\n",
            "          [ 0.0411,  0.0431,  0.0388,  ...,  0.0387,  0.0448,  0.0336],\n",
            "          ...,\n",
            "          [ 0.0492,  0.0409,  0.0331,  ...,  0.0397,  0.0357,  0.0305],\n",
            "          [ 0.0411,  0.0402,  0.0317,  ...,  0.0356,  0.0384,  0.0265],\n",
            "          [ 0.0407,  0.0446,  0.0339,  ...,  0.0371,  0.0380,  0.0290]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0218,  0.0181,  0.0198,  ...,  0.0162,  0.0175,  0.0227],\n",
            "          [ 0.0230,  0.0285,  0.0192,  ...,  0.0114,  0.0233,  0.0295],\n",
            "          [ 0.0232,  0.0232,  0.0129,  ...,  0.0278,  0.0252,  0.0255],\n",
            "          ...,\n",
            "          [ 0.0199,  0.0157,  0.0309,  ...,  0.0325,  0.0213,  0.0275],\n",
            "          [ 0.0256,  0.0190,  0.0224,  ...,  0.0180,  0.0277,  0.0302],\n",
            "          [ 0.0273,  0.0236,  0.0230,  ...,  0.0251,  0.0254,  0.0245]],\n",
            "\n",
            "         [[-0.1264, -0.1301, -0.1328,  ..., -0.1298, -0.1339, -0.1335],\n",
            "          [-0.1217, -0.1234, -0.1264,  ..., -0.1306, -0.1304, -0.1343],\n",
            "          [-0.1237, -0.1287, -0.1388,  ..., -0.1206, -0.1239, -0.1353],\n",
            "          ...,\n",
            "          [-0.1228, -0.1237, -0.1391,  ..., -0.1239, -0.1277, -0.1261],\n",
            "          [-0.1298, -0.1398, -0.1237,  ..., -0.1229, -0.1242, -0.1321],\n",
            "          [-0.1272, -0.1307, -0.1288,  ..., -0.1325, -0.1205, -0.1329]],\n",
            "\n",
            "         [[ 0.0386,  0.0428,  0.0446,  ...,  0.0465,  0.0406,  0.0483],\n",
            "          [ 0.0411,  0.0377,  0.0462,  ...,  0.0367,  0.0534,  0.0557],\n",
            "          [ 0.0489,  0.0411,  0.0470,  ...,  0.0579,  0.0411,  0.0467],\n",
            "          ...,\n",
            "          [ 0.0481,  0.0592,  0.0314,  ...,  0.0361,  0.0388,  0.0478],\n",
            "          [ 0.0479,  0.0401,  0.0436,  ...,  0.0559,  0.0523,  0.0443],\n",
            "          [ 0.0512,  0.0550,  0.0575,  ...,  0.0509,  0.0512,  0.0437]],\n",
            "\n",
            "         [[ 0.0272,  0.0318,  0.0326,  ...,  0.0240,  0.0339,  0.0305],\n",
            "          [ 0.0371,  0.0429,  0.0356,  ...,  0.0295,  0.0397,  0.0351],\n",
            "          [ 0.0327,  0.0413,  0.0335,  ...,  0.0361,  0.0326,  0.0444],\n",
            "          ...,\n",
            "          [ 0.0324,  0.0263,  0.0417,  ...,  0.0375,  0.0383,  0.0332],\n",
            "          [ 0.0496,  0.0492,  0.0427,  ...,  0.0303,  0.0389,  0.0348],\n",
            "          [ 0.0415,  0.0463,  0.0325,  ...,  0.0428,  0.0348,  0.0348]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0226,  0.0270,  0.0212,  ...,  0.0253,  0.0273,  0.0192],\n",
            "          [ 0.0264,  0.0219,  0.0244,  ...,  0.0201,  0.0192,  0.0303],\n",
            "          [ 0.0226,  0.0185,  0.0141,  ...,  0.0381,  0.0285,  0.0247],\n",
            "          ...,\n",
            "          [ 0.0228,  0.0287,  0.0180,  ...,  0.0258,  0.0148,  0.0270],\n",
            "          [ 0.0246,  0.0190,  0.0183,  ...,  0.0299,  0.0223,  0.0273],\n",
            "          [ 0.0367,  0.0283,  0.0183,  ...,  0.0276,  0.0269,  0.0233]],\n",
            "\n",
            "         [[-0.1243, -0.1287, -0.1318,  ..., -0.1255, -0.1305, -0.1304],\n",
            "          [-0.1187, -0.1297, -0.1285,  ..., -0.1225, -0.1319, -0.1231],\n",
            "          [-0.1273, -0.1332, -0.1248,  ..., -0.1229, -0.1272, -0.1258],\n",
            "          ...,\n",
            "          [-0.1253, -0.1211, -0.1375,  ..., -0.1296, -0.1391, -0.1294],\n",
            "          [-0.1304, -0.1340, -0.1213,  ..., -0.1126, -0.1338, -0.1296],\n",
            "          [-0.1191, -0.1290, -0.1237,  ..., -0.1228, -0.1288, -0.1299]],\n",
            "\n",
            "         [[ 0.0437,  0.0414,  0.0455,  ...,  0.0375,  0.0391,  0.0423],\n",
            "          [ 0.0496,  0.0504,  0.0493,  ...,  0.0405,  0.0418,  0.0447],\n",
            "          [ 0.0468,  0.0552,  0.0391,  ...,  0.0397,  0.0407,  0.0430],\n",
            "          ...,\n",
            "          [ 0.0471,  0.0437,  0.0454,  ...,  0.0433,  0.0366,  0.0420],\n",
            "          [ 0.0466,  0.0536,  0.0592,  ...,  0.0506,  0.0519,  0.0532],\n",
            "          [ 0.0534,  0.0529,  0.0547,  ...,  0.0452,  0.0525,  0.0510]],\n",
            "\n",
            "         [[ 0.0333,  0.0359,  0.0274,  ...,  0.0259,  0.0257,  0.0302],\n",
            "          [ 0.0459,  0.0374,  0.0350,  ...,  0.0305,  0.0408,  0.0333],\n",
            "          [ 0.0471,  0.0345,  0.0325,  ...,  0.0441,  0.0322,  0.0317],\n",
            "          ...,\n",
            "          [ 0.0511,  0.0505,  0.0411,  ...,  0.0494,  0.0485,  0.0337],\n",
            "          [ 0.0411,  0.0424,  0.0476,  ...,  0.0403,  0.0331,  0.0336],\n",
            "          [ 0.0468,  0.0398,  0.0405,  ...,  0.0319,  0.0390,  0.0333]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0254,  0.0282,  0.0183,  ...,  0.0241,  0.0197,  0.0238],\n",
            "          [ 0.0237,  0.0200,  0.0325,  ...,  0.0223,  0.0294,  0.0242],\n",
            "          [ 0.0299,  0.0270,  0.0273,  ...,  0.0210,  0.0222,  0.0212],\n",
            "          ...,\n",
            "          [ 0.0282,  0.0320,  0.0284,  ...,  0.0231,  0.0249,  0.0196],\n",
            "          [ 0.0233,  0.0259,  0.0252,  ...,  0.0177,  0.0292,  0.0291],\n",
            "          [ 0.0278,  0.0205,  0.0209,  ...,  0.0247,  0.0219,  0.0245]],\n",
            "\n",
            "         [[-0.1246, -0.1272, -0.1279,  ..., -0.1259, -0.1320, -0.1380],\n",
            "          [-0.1240, -0.1225, -0.1232,  ..., -0.1273, -0.1336, -0.1237],\n",
            "          [-0.1265, -0.1235, -0.1358,  ..., -0.1225, -0.1277, -0.1291],\n",
            "          ...,\n",
            "          [-0.1220, -0.1264, -0.1349,  ..., -0.1245, -0.1238, -0.1343],\n",
            "          [-0.1261, -0.1245, -0.1294,  ..., -0.1303, -0.1281, -0.1281],\n",
            "          [-0.1261, -0.1234, -0.1307,  ..., -0.1215, -0.1257, -0.1278]],\n",
            "\n",
            "         [[ 0.0431,  0.0397,  0.0349,  ...,  0.0361,  0.0450,  0.0449],\n",
            "          [ 0.0410,  0.0406,  0.0480,  ...,  0.0437,  0.0455,  0.0438],\n",
            "          [ 0.0472,  0.0469,  0.0451,  ...,  0.0526,  0.0484,  0.0431],\n",
            "          ...,\n",
            "          [ 0.0572,  0.0396,  0.0397,  ...,  0.0497,  0.0486,  0.0451],\n",
            "          [ 0.0432,  0.0489,  0.0530,  ...,  0.0468,  0.0517,  0.0507],\n",
            "          [ 0.0534,  0.0516,  0.0548,  ...,  0.0569,  0.0526,  0.0517]],\n",
            "\n",
            "         [[ 0.0301,  0.0311,  0.0260,  ...,  0.0368,  0.0284,  0.0324],\n",
            "          [ 0.0407,  0.0320,  0.0384,  ...,  0.0319,  0.0367,  0.0386],\n",
            "          [ 0.0463,  0.0437,  0.0441,  ...,  0.0311,  0.0335,  0.0289],\n",
            "          ...,\n",
            "          [ 0.0419,  0.0472,  0.0390,  ...,  0.0302,  0.0403,  0.0326],\n",
            "          [ 0.0440,  0.0410,  0.0372,  ...,  0.0419,  0.0371,  0.0361],\n",
            "          [ 0.0437,  0.0407,  0.0412,  ...,  0.0412,  0.0391,  0.0324]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0237,  0.0178,  0.0215,  ...,  0.0183,  0.0171,  0.0267],\n",
            "          [ 0.0288,  0.0239,  0.0251,  ...,  0.0314,  0.0246,  0.0223],\n",
            "          [ 0.0321,  0.0240,  0.0240,  ...,  0.0315,  0.0235,  0.0247],\n",
            "          ...,\n",
            "          [ 0.0246,  0.0228,  0.0196,  ...,  0.0251,  0.0171,  0.0225],\n",
            "          [ 0.0246,  0.0298,  0.0286,  ...,  0.0194,  0.0228,  0.0227],\n",
            "          [ 0.0261,  0.0249,  0.0226,  ...,  0.0282,  0.0235,  0.0278]],\n",
            "\n",
            "         [[-0.1282, -0.1266, -0.1364,  ..., -0.1337, -0.1340, -0.1277],\n",
            "          [-0.1249, -0.1273, -0.1179,  ..., -0.1311, -0.1260, -0.1308],\n",
            "          [-0.1203, -0.1345, -0.1272,  ..., -0.1202, -0.1273, -0.1276],\n",
            "          ...,\n",
            "          [-0.1272, -0.1272, -0.1277,  ..., -0.1324, -0.1277, -0.1286],\n",
            "          [-0.1330, -0.1303, -0.1214,  ..., -0.1301, -0.1160, -0.1298],\n",
            "          [-0.1234, -0.1275, -0.1280,  ..., -0.1251, -0.1286, -0.1232]],\n",
            "\n",
            "         [[ 0.0454,  0.0405,  0.0482,  ...,  0.0384,  0.0468,  0.0480],\n",
            "          [ 0.0443,  0.0602,  0.0447,  ...,  0.0496,  0.0454,  0.0488],\n",
            "          [ 0.0415,  0.0540,  0.0462,  ...,  0.0517,  0.0497,  0.0457],\n",
            "          ...,\n",
            "          [ 0.0383,  0.0546,  0.0541,  ...,  0.0474,  0.0488,  0.0542],\n",
            "          [ 0.0435,  0.0551,  0.0415,  ...,  0.0519,  0.0491,  0.0470],\n",
            "          [ 0.0485,  0.0491,  0.0539,  ...,  0.0491,  0.0555,  0.0458]],\n",
            "\n",
            "         [[ 0.0353,  0.0275,  0.0245,  ...,  0.0266,  0.0262,  0.0318],\n",
            "          [ 0.0486,  0.0359,  0.0444,  ...,  0.0367,  0.0420,  0.0327],\n",
            "          [ 0.0453,  0.0326,  0.0288,  ...,  0.0305,  0.0320,  0.0346],\n",
            "          ...,\n",
            "          [ 0.0450,  0.0425,  0.0364,  ...,  0.0364,  0.0437,  0.0395],\n",
            "          [ 0.0425,  0.0416,  0.0389,  ...,  0.0378,  0.0409,  0.0351],\n",
            "          [ 0.0399,  0.0335,  0.0414,  ...,  0.0434,  0.0332,  0.0352]]]],\n",
            "       grad_fn=<MkldnnConvolutionBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpgj2fVnm_aG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11b48e37-7f05-4cc9-dd78-a568b7fd8734"
      },
      "source": [
        "output.min()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.1624, grad_fn=<MinBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSO71epW2-4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9324cc4a-7360-4876-8555-834b1fdd0802"
      },
      "source": [
        "output.max()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0825, grad_fn=<MaxBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxosKwgsEyMv",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlENblcH3qNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1hA_Zm-E4JI",
        "colab_type": "text"
      },
      "source": [
        "# Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VMYLzYeE514",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}